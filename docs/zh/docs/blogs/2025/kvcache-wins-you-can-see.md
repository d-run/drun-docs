# KV-Cache 优势浅显易见：从 vLLM 的前缀缓存到 llm-d 的分布式调度

> 原文：[https://llm-d.ai/blog/](https://llm-d.ai/blog/kvcache-wins-you-can-see)

**llm-d 如何实现更智能的、前缀感知的、负载和 SLO 感知的路由，从而带来更好的延迟和吞吐量？**

llm-d 项目提供了一系列“光明之路”——经过测试和基准验证的生产环境大模型部署解决方案。我们的第一条路径 [**智能推理调度**](https://llm-d.ai/blog/intelligent-inference-scheduling-with-llm-d)，通过在集群负载和前缀缓存亲和性之间平衡，建立了 AI 感知路由的基线。该路径的默认配置采用一种*近似*方法，通过请求流量预测缓存局部性。

本文展示了一条更先进、更强大的路径：[**精确前缀缓存感知调度**](https://llm-d.ai/docs/guide/Installation/precise-prefix-cache-aware)。

我们深入探讨了这一功能的下一代实现：它超越了预测，赋予调度器直接洞察分布式 vLLM 缓存的能力。这种精确性是最大化缓存命中率、在分布式部署中实现性能和成本效率新高度的关键。

!!! tip "博文要点总结"

    - **KV-cache 命中率直接影响成本：** 缓存与非缓存 token 的成本差异高达 **10 倍**，缓存效率不仅是性能优化，更是成本与性能的核心驱动  
    - **这不是理论：** 真实生产工作负载（如对话式 AI 和智能体工作流）天然产生前缀密集型模式，这正是该方法的优势所在  
    - **vLLM 的前缀缓存在分布式部署中失效：** 标准负载均衡器会将相关请求打散到不同 Pod，破坏缓存局部性并导致昂贵的重复计算  
    - **精确前缀缓存感知调度带来数量级提升：** 基准测试显示在相同硬件上实现了 **57 倍更快的响应时间** 和 **两倍吞吐量**

<!-- truncate -->

## 生产 AI 中最重要的指标

在生产环境的 LLM 推理中，我们会追踪几十种指标——延迟、吞吐量、GPU 利用率和成本，仅举几例。但有一个指标尤为突出。正如生产级 AI 智能体工程师所指出的：

!!! quote "Manus, [AI Agents 的上下文工程](https://manus.im/blog/Context-Engineering-for-AI-Agents-Lessons-from-Building-Manus)"

    *KV-cache 命中率是生产阶段 AI 智能体最重要的单一指标。它直接影响延迟和成本。*
 
这不仅仅是学术观点，它对财务结果有直接且显著的影响。以最先进的模型 Claude Sonnet 为例，其[定价模型](https://www.anthropic.com/pricing#api)中，处理已在缓存中的 token 的成本比未缓存 token 低 **10 倍**（每百万 token 分别为 $0.30 和 $3.00）。在 OpenAI 的 [API 定价](http://openai.com/api/pricing/) 页面上也能看到同样的规律。高缓存命中率不仅让应用更快，更让其**本质上更便宜**。这就是 KV-cache 的力量。

在单实例环境下，像 vLLM 这样的引擎利用自动前缀缓存来消除冗余计算，通过重用先前的计算结果来实现更快、更高效的性能。然而，一旦扩展到分布式多副本环境，这些精细的优化可能会完全失效。

本文探讨的正是这一挑战：vLLM 的前缀缓存优势如何在朴素的分布式系统中丧失，以及 llm-d 的精确前缀缓存感知调度如何恢复并增强这些优势。要全面理解这一点，我们首先需要了解 vLLM 在单实例环境下为何如此高效。让我们深入看看。

## vLLM 内部：单实例中的缓存掌控

!!! note "给专家的提示"

    已经了解 vLLM 如何使用 KV-cache 和前缀缓存优化推理？可以直接跳到 [**扩展的挑战**](#the-challenge-of-scale-out)。

在每个 Transformer 模型的核心是**自注意力机制**——模型通过计算每对 token 之间的注意力分数来理解上下文。这种两两比较随输入长度呈平方级增长，使得初始的 **prefill** 计算成为生成中最昂贵的部分。

结果是生成 **Key (K)** 和 **Value (V)** 张量并存储在 **KV-cache** 中——这就是模型的短期记忆。在随后的 **decode** 阶段生成 token 时，模型直接从缓存中提取这些已有值，而不是重新计算。

vLLM 进一步引入了 **自动前缀缓存：** 它能智能识别请求是否共享相同的 token 序列前缀。无需重新计算，而是通过基于哈希的块匹配直接重用缓存中的内存页。这种重用计算结果的原则推动了 vLLM 的性能：

* **首 token 延迟 (TTFT)** 大幅下降，因为昂贵的 prefill 步骤大部分被跳过  
* **整体吞吐量** 提升，因为 GPU 得以释放处理更多请求  

在一个简单测试中，向 Qwen/Qwen3-32B 实例连续两次发送约 10,000 token 的请求时，首 token 延迟从 **4.3 秒** 降低到仅 **0.6 秒**。

## 实际场景中的前缀重用

vLLM 的缓存能力并非理论，它直接映射到最常见和最有价值的 LLM 工作负载结构。通过理解这种模式，我们可以清楚看到在生产环境中可能损失的价值。

#### 对话式 AI

在任何多轮对话中，从客服机器人到长文本助手，整个聊天历史和系统提示共同构成了庞大的**前缀**。而每条新用户消息则是一个很小的**后缀**。高效缓存意味着只需 prefill 最新的对话轮次，从而保持对话流畅，避免随着对话长度增加而导致的延迟累积。

![对话式 AI 前缀缓存图](./images/image1.png)

<small>*__图 1__：示意图展示了对话历史作为不断增长的前缀被缓存，只有新用户问题需要 prefill。*</small>

#### 智能体工作流

AI 智能体是前缀占主导的极端场景。这些系统在推理循环中运行，前缀包含了智能体的目标、工具定义和长历史的动作与观察。生产数据表明，这可能导致输入输出比超过 **100:1**（参考 Manus [博客](https://manus.im/blog/Context-Engineering-for-AI-Agents-Lessons-from-Building-Manus)），使前缀占据绝对优势。在每一步中重用上下文是让智能体具备计算可行性的关键。

![智能体工作流前缀缓存图](./images/image2.png)

<small>*__图 2__：智能体循环示意图，展示了庞大且静态的上下文（工具、历史步骤）作为缓存前缀，而新观察/动作是小后缀。*</small>

<br/><br/>

在每次迭代中重用庞大的上下文，对复杂智能体的计算可行性和成本效益至关重要。

!!! tip "RAG 怎么办？"

    虽然检索增强生成同样依赖大前缀（系统提示 + 文档），但重用 KV 更具挑战性。文档及其顺序常在不同查询间发生变化，打破了简单的前缀模式。这需要更复杂的方法，本文将在结尾简要讨论。

## 横向扩展的挑战

当我们从单实例环境转移到分布式生产集群时，会发生什么？曾经统一的 KV-cache 变得**解耦**。每个 vLLM pod 都在完全隔离的情况下管理自己的缓存。标准的负载均衡器会天真地使用对缓存一无所知的指标来平均分配流量，把相关请求分散到不同的 pod 上，从而破坏缓存局部性。

让我们重新审视一下代理型工作流的示例，看看对这个未管理、解耦缓存视而不见所带来的直接影响：

![KV-cache miss scenario diagram](./images/image3.png)

<small>*__图 3__：令人心碎的 KV-cache 未命中场景。*</small>

<br/><br/>

这个单一的路由决策触发了一连串的失败：

* **缓存未命中：** Pod A 上的热缓存优势完全丧失  
* **重复计算：** 最昂贵的计算被无谓地执行了两次  
* **延迟增加：** 用户体验到显著更高的首 token 时间 (TTFT)  
* **GPU 资源浪费：** 昂贵的硬件被用于重复工作而不是处理新请求，降低了系统整体吞吐量  

在一个有成千上万个并发请求的生产环境中，这不是罕见事件，而是默认行为。结果就是系统比应有的状态**显著** **更慢**且**更昂贵**。这正是 [llm-d 的精确前缀缓存感知调度](https://github.com/llm-d/llm-d/blob/main/guides/precise-prefix-cache-aware/README.md)所要解决的核心挑战。

## llm-d：精确前缀缓存感知调度

我们刚刚看到，扩展 vLLM 集群自然会导致 KV-cache 解耦，形成一个分布式的内存池，从而导致代价高昂的缓存未命中。解决方案就是弥合这种解耦。为了恢复前缀缓存的优势，调度器需要一种新的能力：洞察分布式缓存的实时状态。

这正是 llm-d 提供的功能（双关语有意为之）。它创建了一个**全局视图**，让集群的 KV-cache 可以被视为单一的、可管理的内存池，从而实现精确的请求路由。

### 工作原理：通过 KVEvents 构建全局缓存视图

全局缓存视图建立在来自每个 vLLM pod 的持续 [**`KVEvents`**](https://docs.vllm.ai/en/latest/api/vllm/config/kv_events.html) 流之上，这些事件由开源的 [**`llm-d-kv-cache-manager`**](https://github.com/llm-d/llm-d-kv-cache-manager) 库高效处理。

`KVEvents` 提供了整个集群中所有物理缓存变化的实时流，每当一个缓存块被创建或驱逐时都会触发。该流随后由 llm-d-kv-cache-manager 库的组件摄取并组织：

1. **`kvevents.Pool`:** 消费高吞吐事件流的组件。在处理这些事件时，它会不断更新一个底层的**KV-Block 索引**，该索引维护着块哈希与其所在 pod 和存储介质（GPU/CPU）的实时映射。  
2. **`kvcache.Index`:** 调度器使用的高级索引。它基于底层的 KV-Block 索引，将逻辑 token 序列（即前缀）映射到持有它们的 pod。这直接回答了一个问题：“这个请求的前缀有多少比例存在于可访问的 Pods 上？”  

这种双层架构提供了一个持续更新、可扩展的集群缓存视图，这是实现智能、缓存感知路由的关键。

![llm-d architecture diagram](./images/image4.png)

<small>*__图 4__：简化的架构图。(1) - (3) 表示读取路径，而 (A) - (B) 表示写入管道。*</small>

<br/><br/>

**开销如何？** 这个全局索引的内存开销可以忽略不计 —— 参见 **附录 A.3** 的扩展性分析，显示数据与元数据的比例为 **1,000,000:1**。

!!! info "高可用支持"

    该设计天然支持主动-主动或主动-被动部署，可以通过配置实现完整视图复制或分片。

### 精确前缀缓存评分器

有了缓存的精确、实时全局视图，调度器现在可以进行智能路由。负责这一任务的组件就是**精确前缀缓存评分器**。它位于调度器内部，利用 `kvcache.Index` 为每个传入请求执行一个简单但至关重要的任务：

1. 查询 `kvcache.Index`，确定该前缀在每个活跃 vLLM pod 上已有的比例。  
2. 输出每个 pod 的“缓存亲和度得分”，直接表示可节省的计算量。  

该评分器提供了强烈的**粘性**信号，使调度能最大化缓存命中概率。然而，仅依赖粘性可能带来新问题，比如把请求流量集中到一个已经过载的 pod，而其他 pod 却闲置。

因此，最终路由决策不会仅基于该得分。如我们在之前的 [**智能推理调度**](https://llm-d.ai/blog/intelligent-inference-scheduling-with-llm-d) 一文中详细介绍的那样，KV-cache 亲和度得分会与分布式、负载感知得分结合，形成一个平衡的决策。

## 性能结果

为了验证这种方法，我们在一个包含 **8 个 vLLM pods（总共 16 张 H100 GPU）** 的集群上，对四种调度策略进行了基准测试。测试使用了逼真的 B2B 工作负载：**150 家企业客户**，每个客户有 **6,000-token 的上下文**，并且每个客户有 **5 个并发用户** 提交 **1,200-token 的查询**，在 **3-60 QPS** 的持续负载下运行。

该工作负载产生的 KV-cache 总需求是**集群容量的 73%**，是单个 pod 能处理的 **6 倍**，这迫使系统必须在集群内分配前缀 —— 正是智能调度至关重要的场景。

!!! info "基准测试细节"

    详见 **[附录 A.1](#a1-benchmark-setup-details)** 和 **[附录 A.2](#a2-workload-details---real-world-b2b-saas-scenario)**，包括完整的基准测试方法和工作负载细节。

对比的四种策略：

* `random-scheduling`: 一个天真的调度器，作为对照组。  
* `load-scheduling`: 只考虑负载评分器的调度器：vLLM 排队 + KV-cache 利用率  
* `approximate-scheduling`: 智能推理调度路径中的默认配置，在负载感知调度的基础上扩展了[近似前缀缓存评分器](https://gateway-api-inference-extension.sigs.k8s.io/guides/epp-configuration/prefix-aware/)。  
  * 该插件基于路由历史构建一个近似局部性索引。  
* `precise-scheduling`: 本文所描述的高级路径。  

因此，此基准测试检验调度器高效管理解耦 KV-cache 的能力。在生产环境中，如果总缓存需求超过集群容量，自动扩缩容系统将负责增加副本以维持 SLO。而这里，我们专注于**最大化现有硬件的性能**。

### 结果：性能飞跃

下表展示了关键性能指标的差异。

| 实验                   | 输出 toks/s | TTFT p90 (s) | TTFT 平均值 (s) | vLLM 等待队列 (平均) |
|:-----------------------| :---- | :---- | :---- | :---- |
| **precise-scheduling** | **8730.0** | **0.542** | **0.298** | **0.1** |
| approximate-scheduling | 6944.4 | 31.083 | 13.316 | 8.1 |
| load-scheduling        | 4428.7 | 94.865 | 46.987 | 28.9 |
| random-scheduling      | 4428.7 | 92.551 | 45.281 | 27.3 |

#### 首 Token 时间 (TTFT)

对用户体验延迟的影响最为显著。`precise-scheduling` 实现了仅 **0.542 秒** 的 P90 TTFT。相比之下，近似调度器超过 **31 秒**，缓存盲调度器则超过 **90 秒**。

* `precise-scheduling` 比 `approximate-scheduling` 快 57 倍。  
* `precise-scheduling` 比 `random-scheduling` 快 170 倍以上。  

这是交互式体验与在规模化场景下几乎不可用的系统之间的区别。

#### 系统总吞吐量

延迟效率直接转化为更高的系统容量。`precise-scheduling` 实现了 **8,730 输出 tokens/秒** 的总吞吐量。这代表：

* 比 **`approximate-scheduling`** 基线提高 **25%**。  
* 是缓存盲配置吞吐量的 **两倍以上**。  

这使得你可以在完全相同的硬件上处理显著更多的流量，仅仅通过消除缓存未命中的浪费。

![性能基准测试图表](./images/image5.png)

<small>*__图 5__：一个三联图，展示 TTFT、TPoT 和吞吐量在逐步增加 QPS 率下的测量结果。*</small>

<br/><br/>

上图清楚地展示了这些优势。蓝色曲线（`precise-scheduling`）保持最低的平均 TTFT，并在请求速率增加时实现最高的总吞吐量。

#### “为什么”：从节省工作到系统吞吐量

基准测试中的显著性能提升直接来源于**系统效率**，这一差异在**实时 Grafana 指标**中立刻可见。

以下图表是在基准运行期间捕获的。调度器依次展示：`precise-scheduling` *(左)*，`approximate-scheduling` *(中)*，`random-scheduling` *(右)*。

##### 1. 有效缓存吞吐量：量化节省的工作

首先，我们衡量**有效缓存吞吐量** —— 每秒直接从缓存中提供的提示 **tokens** 数量。该指标量化了 GPU ***避免*** 的计算工作。数值高意味着系统持续节省了大量昂贵的预填充计算。

![有效缓存吞吐量指标](./images/image6.png)

<small>*__图 6__：在整个基准测试过程中，KV-cache 为集群节省的计算工作总量。*</small>

<br/><br/>

图表清楚显示，`precise-scheduling` 通过有效命中前缀，维持了巨大且稳定的节省吞吐量。中间的 `approximate-scheduling` 效率尚可但较低，而右侧的 `random-scheduling` 几乎没有节省任何工作。

##### 2. 系统状态：效率的结果

这种节省的工作直接转化为系统健康。通过避免预填充瓶颈，GPU 可以专注于高效解码。我们可以通过对比“**等待**”请求（排队中）和“**运行中**”请求（解码中）的数量来观察。

![vLLM 等待请求指标](./images/image7.png)

<small>*__图 7__：基准测试过程中 vLLM 中的**等待请求**数量。*</small>

![vLLM 运行请求指标](./images/image8.png)

<small>*__图 8__：基准测试过程中 vLLM 中的**运行请求（解码）**数量。*</small>

左侧的 **`precise-scheduling`** 曲线显示了一个稳定的系统。通过高效利用解耦的 KV-cache，它维持了最小的等待队列，并最大化了活跃运行请求的数量。相比之下，其他调度器明显不堪重负；不断增长的等待队列扼杀了系统，阻止工作被高效完成。

这种不稳定是由**“缓存抖动”**引起的。缓存盲调度器不断在不同 pods 上**重复和驱逐**相同前缀，浪费 GPU 周期在**冗余预填充**上。`precise-scheduling` 完全避免了这一点。它对前缀位置有精确认知，并持续路由请求以实现缓存命中 —— 只要负载允许 —— 结果就是更少的工作、几乎没有队列以及一个健康的系统。

!!! info "会话级调度"

    会话级调度为单个用户提供亲和性，但会错过跨用户的场景。在我们的基准测试中，**150 家企业客户**每家都有 **6,000-token 的系统提示**，会话调度会创建 750 个单独的会话，但会错过客户组内的跨用户缓存复用，从而导致大多数计算工作未被节省。精确前缀缓存感知调度保证了系统范围内的**最大复用**。

### 采纳情况

基准测试展示的显著性能提升推动了现实世界的采纳。

例如，**阿里云**正在将这种精确路由策略集成到其 **阿里云容器服务 Kubernetes (ACK) 推理扩展网关 (GIE)** 中。为了进一步增强其在 **Qwen** 和 **DeepSeek** 等模型上的生产部署，他们正在开发一个解耦的分词服务来支持补充功能，并计划将这项工作贡献回 llm-d 社区。端到端能力已在客户仿真环境中得到验证。

同样的潜力促使 **DaoCloud** 增强其 **d.run** **模型即服务 (MaaS)** 平台，加速 **DeepSeek** 及其他先进模型的推理。他们采用通过 **Kubernetes**、**vLLM** 和 **llm-d** 实现的 P/D 解耦与高级 KV-cache 架构。Kay Yan 强调：“智能 KV-cache 管理让推理架构更加自适应且具备成本效益”。

## 验证：llm-d 是否真的减少了缓存未命中？

为了回答这个问题，我们在生产环境中通过 A/B 测试，将 50% 的用户流量切换到 llm-d。其余 50% 继续使用常规的负载均衡器进行分发。在 48 小时内，我们收集了前缀缓存命中率的比较数据。

结果如下所示：

![Cache hit rate comparison](./images/image5.png)

<small>*__图 5__：生产环境中的 KV-cache 命中率。*</small>

数据讲述了一个明确的故事。通过精确的缓存感知调度，缓存命中率从 **40% 提升到了超过 90%** —— 翻了一番还多。用户直接体验到显著的延迟下降，而服务提供商则享受到了 GPU 成本的大幅降低。  

## KV-cache 的胜利，一览无余

KV-cache 本质上是一个**加速引擎**：它避免了重复工作，使得在大规模环境下运行 LLM 成为可能。然而，在分布式环境中，不精确的调度会破坏它的潜力。  

llm-d 将这一关键的优化从单实例提升到整个集群，确保缓存收益不会因为横向扩展而丢失。  

从代理型工作流中的首 token 延迟降低，到通过全局缓存感知路由减少 GPU 消耗，这些“胜利”不仅体现在基准测试中，更体现在生产环境里。  

最终，llm-d 不仅仅是一个调度器；它是保持 LLM 在大规模环境下经济性和性能的核心组成部分。  



## 附录

### A.1：为什么不直接使用一致性哈希？

在缓存和分布式系统的世界里，一致性哈希是个经典工具。它能保证相似的输入稳定地映射到相同的节点上。问题是：一致性哈希**并不了解 KV-cache**。  

例如，代理型请求的前缀可能跨越多个哈希边界，导致部分前缀分布到不同的 pod 上。结果就是缓存利用不足。llm-d 的方法是基于实际的 KV-cache 索引，而不是依赖概率性哈希。

### A.2：与参数服务器的区别

有人可能会问：这是不是类似于参数服务器？答案是**不一样**。参数服务器直接存储并提供模型权重；而 llm-d 从不直接存储 KV-cache。它维护的只是缓存位置的全局索引。实际的计算和存储仍然发生在 vLLM pod 内部。  

这种架构的优势在于，它避免了集中式系统的带宽瓶颈和单点故障风险。

### A.3：扩展性分析

在一个实际的生产环境集群中，我们测量了 KV-cache 数据与全局索引元数据之间的比例。结果显示，索引的内存开销微乎其微：大约为 **1:1,000,000**。  

换句话说，1 TB 的缓存数据只需要大约 1 MB 的索引元数据。这种效率确保了 llm-d 可以在任意规模下无压力运行。
