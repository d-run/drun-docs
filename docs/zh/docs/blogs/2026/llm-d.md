# llm-d 智能代理运行时未来指北

> 本文素材源自 [llm-d 社区公开的北极星文档](https://docs.google.com/document/d/1C3wRYLSZ9GPT2454MvDC6-8T_gvMPsPL_-ISVTZj4FU/edit?tab=t.0)

[llm-d](https://llm-d.ai/) 是 Kubernetes 原生的高性能、分布式推理框架，让你轻松在多种硬件上大规模部署生成式 AI 模型，快速跑出最先进的推理水平！

本文规划了未来 llm-d 的架构演进——从通用的、**以请求为中心** 的推理引擎，迈向专门的 **智能代理运行时**，支持框架向 **工作负载感知推理** 的转型。

llm-d 社区认为，下一阶段的效率飞跃，不会来自对单次请求的微小优化，而在于对 **有状态工作负载** 的整体协调和调度。

本文是智能代理工作负载演进的总纲，明确了核心原则和技术支柱，是工程路线图与智能代理生态新需求之间的战略桥梁。

**“用例优先”原则**：摒弃抽象优化，强调 **工作负载驱动的工程设计** 。本文提出的每个架构支柱，都针对智能代理模式中实际遇到的“痛点”。

通过将路线图与这些行为紧密绑定，llm-d 能真正解决生产环境中代理的核心瓶颈——成本、延迟、状态管理——而不仅仅是追求原始 token 吞吐量。

## 动机：智能代理的“隐形成本”

当前最先进（SOTA）的推理框架已经优化了 **孤立请求** 的处理。通过将 KV-cache 当作托管内存层，
提升了 TTFT（首次 token 时间）和 TPOT（每个输出 token 时间）。然而，AI 行业正在从简单聊天互动，迈向 **智能代理工作流**
（如思维树 Tree-of-Thought、LLM 作为裁判的群体智能、多步工具调用）。

在这些工作流中，推理请求不再孤立，它们是 **程序的一部分** 。

- **状态重叠（State Overlap）**：智能代理工作流在各个推理分支之间存在大量 KV-cache 前缀共享。
- **调度盲点（Scheduling Blindness）**：现有调度器注重请求级公平，但智能代理程序关心的是 **整体程序完成时间** 。一个分支的延迟可能阻塞整个智能循环。

要实现效率的跃升，推理栈必须理解智能代理程序的 **意图** 和 **结构** 。需要在应用提供提示与上下文信息时，构建 llm-d 的上下文感知基础设施与执行能力。

## 缺口：为什么主流推理尚未真正原生支持智能代理

尽管 llm-d 已在通用 KV-cache 推理上打下坚实基础，但仍存在以下差距，阻碍其向智能代理运行时演进：

1. **指标不匹配**：我们关注请求级延迟，而智能代理需要 **整体程序延迟** 和 **任务完成吞吐量** 。现有系统无法区分属于同一任务的请求。
2. **被动状态管理**：当前 KV-cache 主要按需加载，而智能代理是可预测的；“计划”完成后，“执行”步骤随之而来。但我们缺乏 **主动调度 KV-cache** 的 API。
3. **上下文平等对待**：推理栈将所有 KV-cache 块视为同等价值，而在智能代理中，“系统提示”或“工具定义”是高价值/长生命周期（静态上下文），而“思维链”可能短暂（临时上下文）。我们缺少 **上下文标记（Context Taints）** 来实现差异化管理。
4. **分解带来的延迟**：KV 分解虽然有利于扩展，但计算节点与 KV-cache 的物理距离可能引入延迟，而智能代理循环对顺序调用高度敏感。

## 愿景：构建上下文感知的推理生态

社区的未来目标是让 llm-d 成为 **以 KV-cache 为核心的智能代理协调平台** 。

不再将推理栈视为接收文本的黑盒，而是打造 **理解智能代理程序的生态系统** 。

该愿景基于三大原则和五个核心架构支柱：

- **框架提供提示**：智能代理层为请求附加程序 ID、优先级与上下文生命周期信息。
- **llm-d 管理全生命周期**：推理栈根据这些提示，智能决定 KV-cache 的存储、迁移和分支优先级。

    这要求对智能代理工作流元素有清晰感知，并与 KV-cache 关联。

- **全局视角**：KV-cache 不再局限于单节点或会话，而是跨集群的全局资源，指导并伴随代理逻辑。

## 五大执行支柱

### I. 程序感知调度器

将调度器从管理“请求队列”升级为管理 **程序执行图** 。调度器需理解代理依赖关系（如“阻塞步骤”或“可并行分支”），识别关键路径，优化资源分配，避免整个工作流停滞。

### II. 主动状态管理

改变数据流方向：不再固定请求到状态节点，而是 **将状态移动到可用计算节点** 。借助 P2P NIXL 和共享存储，KV-cache 成为统一、灵活的资源池。

此支柱以 KV-cache API 扩展控制面，例如：

- 迁移到层（Move to tier）
- 层间固定（Pin in tier）
- 从层驱逐（Evict from tier）

从而支持主动调度逻辑。

### III. 语义化 KV-Cache

将 KV-cache 从原始张量转为 **带类型的数据结构**，**捕获工作负载语义和上下文标记** 。
智能代理的状态类型包括 `SystemPrompt`、`ToolDefinition`、`ReasoningBranch`、`ExecutionCheckpoint`。

通过语义元数据关联缓存块，能够采用差异化的策略，例如“始终复制 `ToolDefinitions`”，“从不卸载 `Checkpoints`”，提升缓存管理智能化。
此支柱以 KV-cache 污点化 API 扩展控制面，采用特殊的数据结构捕捉语意关联。

### IV. 上下文感知 KV-cache 策略

替代简单的 LRU 淘汰策略，实现 **显式生命周期管理** 。
智能代理框架可标记高价值上下文（技能/工具），并允许临时思维块提前淘汰，实现精确内存控制。

### V. 工作负载驱动的基准与模拟

建立智能代理模式与基础设施的持续反馈循环。通过 **典型工作负载模拟器** ，评估核心指标：程序完成时间、零重算命中率、语义上下文类型的缓存利用率。

模拟框架支持参数配置，如前缀重叠率、分支因子、工具调用频率与委托深度，实现基于实际生产摩擦点的优化验证。

## 用例驱动开发

llm-d 关注三大典型模式，引领下一代 AI 应用：

### 用例 1：思维树/分支探索

**模式：** 从共享上下文生成 N 条并行推理路径，评估并扩展最佳候选结果。

**示例：** 代码生成，从同一问题探索 8 种实现方案，评估后扩展前 2。

**特性：**

- 大量共享前缀（30-100k tokens）
- 分支在某个点分叉，但共享大部分上下文
- 并行执行，需汇总结果再进行下一步
- 分支上下文是短暂的，而共享上下文是稳定的

!!! note

    这种用例适合强化学习（RL）。

### 用例 2：多轮工具使用/ReAct 循环

**模式：** 代理迭代推理 → 调用工具 → 处理结果 → 再推理，每一轮基于前一轮的上下文。

**示例：** 数据分析：加载数据 → 描述统计 → 推理异常值 → 绘制分布 → 综合发现。

**特性：**

- 顺序依赖，上一轮的输出是下一轮的输入
- 高上下文复用，工具定义每轮重复（10-30k tokens）
- 可预测模式：工具调用后通常跟随下一轮推理
- 生命周期混合：工具定义持久，临时思维短暂

### 用例 3：分层代理群/委托

**模式：** 父代理生成 N 个子代理，共享指令/数据，独立工作内存，完成后汇总结果。

**示例：** 论文评分：监督者生成 50 个评分器，每人评估一篇论文，返回分数汇总。

**特性：**

- 大量上下文共享（90-95%）
- 批处理语义：完成时间由最慢子代理决定
- 分层结构：子 → 父汇总
- 两阶段执行：子并行，父汇总

### 用例 4：跨多代理工作流

**模式：** 多个代理协作，按顺序、并行或 fork/join 执行。

**示例：** 投资分析：研究员 → 财务分析师 → 投资建议代理。

**特性：**

- 跨代理工作负载身份
- KV-cache 重用：前置代理生成内容成为下游上下文
- 执行依赖：每个代理需前置输出或上下文
- 程序级优化：延迟和内存性能在代理间衡量

## llm-d 的目标

| 目标编号 | 目标描述 | KV 分解路线图 |
|--------|---------|--------------|
| 目标 1 | 以程序为中心进行优化（超越 TTFT） | 优化整个智能代理程序，降低端到端延迟，提高任务吞吐量 |
| 目标 2 | 集群级零重算 | 建立“写一次，集群可读”不变量，任何节点已有语义块无需重复计算 |
| | | - **共享存储卸载**：实现集群范围 KV-cache 共享<br>- **P2P KVConnector**：跨层直接传输 KV 内存，避免重复计算<br>- **位置无关 KV-Cache**：允许部分块合并重用，适用于非线性代理流 |
| 目标 3 | 显式语义缓存控制 | 从黑盒淘汰（LRU）转向上下文感知生命周期管理，框架可标记高优先级与临时上下文块 |
| | | - **KV-Cache API**：Pin、Evict、Move、Prefetch<br>- **上下文感知策略**：淘汰逻辑尊重标记信号<br>- **分层 KV-Cache**：实现基于淘汰的卸载策略 |
| 目标 4 | 主动状态分发 | 预测缓存需求，水平扩展时主动复制上下文 |
| 目标 5 | 基准测试与评估 | 引入智能代理模拟器，实现基于实际工作负载的优化验证 |

## 非 llm-d 的目标

| 编号 | 描述 | KV 分解路线图 |
|-----|---------|--------------|
| 非目标 1 | 管理代理逻辑 | llm-d 保持 **执行引擎**，仅提供接口支持逻辑层（LangChain、LlamaIndex 等）高效驱动 |
| 非目标 2 | 单节点调度器微优化 | 不在单实例内重做 OS 调度器，专注 **集群级调度与编排** |

!!! note

    本文内容如有变更，恕不另行通知。具体请参阅
    [llm-d 公开的文档](https://github.com/llm-d/llm-d)。
