import requests
import json
import csv
from datetime import datetime, timezone
import pandas as pd
from collections import OrderedDict
import yaml
import re
import logging
import os




# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s:%(message)s')

ITEM_HEADER = [
    'åŠŸèƒ½æ¨¡å—',
    'å‘å¸ƒç‰ˆæœ¬',
    'å‘ç‰ˆæ—¶é—´',
    'æ›´æ–°ç±»å‹',
    'ä¸€çº§åŠŸèƒ½',
    'äºŒçº§åŠŸèƒ½',
    'åŸºçº¿å‚æ•°'
]

def get_tenant_access_token(app_id, app_secret):
    url = "https://open.feishu.cn/open-apis/auth/v3/tenant_access_token/internal/"
    payload = {"app_id": app_id, "app_secret": app_secret}
    try:
        resp = requests.post(url, json=payload, timeout=10)
        resp.raise_for_status()
        data = resp.json()
        token = data.get("tenant_access_token")
        if not token:
            logging.error("è·å–tenant_access_tokenå¤±è´¥ï¼Œå“åº”å†…å®¹: %s", data)
            return None
        return token
    except requests.RequestException as e:
        logging.error("è¯·æ±‚tenant_access_tokenå¤±è´¥: %s", e)
        return None

def get_node_info(node_token, tenant_access_token):
    url = "https://open.feishu.cn/open-apis/wiki/v2/spaces/get_node"
    headers = {"Authorization": f"Bearer {tenant_access_token}"}
    params = {"token": node_token, "obj_type": "wiki"}
    try:
        resp = requests.get(url, params=params, headers=headers, timeout=10)
        resp.raise_for_status()
        return resp.json()
    except requests.RequestException as e:
        logging.error("è¯·æ±‚node_infoå¤±è´¥: %s", e)
        return None

def get_sheet_content(app_token, table_id, view_id, tenant_access_token, page_size=100, page_token=None):
    url = f"https://open.feishu.cn/open-apis/bitable/v1/apps/{app_token}/tables/{table_id}/records/search"
    headers = {
        "Authorization": f"Bearer {tenant_access_token}",
        "Content-Type": "application/json"
    }
    payload = {
        "view_id": view_id,
        "page_size": page_size,
    }
    if page_token:
        payload["page_token"] = page_token
    try:
        resp = requests.post(url, headers=headers, data=json.dumps(payload), timeout=10)
        resp.raise_for_status()
        return resp.json()
    except requests.RequestException as e:
        logging.error("è¯·æ±‚sheet_contentå¤±è´¥: %s", e)
        return None

def get_items(data):
    if not data or 'data' not in data or 'items' not in data['data']:
        logging.warning("æ•°æ®æ ¼å¼é”™è¯¯æˆ–æ— itemså­—æ®µ")
        return [ITEM_HEADER]
    items = data['data']['items']
    filter_items = [ITEM_HEADER]
    for item in items:
        fields = item.get('fields', {})
        first_level = ','.join([x.get('text', '') for x in fields.get('ä¸€çº§åŠŸèƒ½', [])]) if fields.get('ä¸€çº§åŠŸèƒ½') else ''
        second_level = ','.join([x.get('text', '') for x in fields.get('äºŒçº§åŠŸèƒ½', [])]) if fields.get('äºŒçº§åŠŸèƒ½') else ''
        baseline = ','.join([x.get('text', '') for x in fields.get('åŸºçº¿å‚æ•°', [])]) if fields.get('åŸºçº¿å‚æ•°') else ''
        version = ''
        if fields.get('ç‰ˆæœ¬') and fields['ç‰ˆæœ¬'].get('value'):
            try:
                version = fields['ç‰ˆæœ¬']['value'][0].get('text', '')
            except (IndexError, AttributeError):
                version = ''
        release_time = fields.get('å‘ç‰ˆæ—¶é—´', {})
        normal_date = None
        if release_time and 'value' in release_time and release_time['value']:
            try:
                timestamp = int(release_time['value'][0]) / 1000
                normal_date = datetime.fromtimestamp(timestamp, timezone.utc).strftime('%Y-%m-%d')
            except (ValueError, IndexError, TypeError):
                normal_date = None
        row = [
            fields.get('åŠŸèƒ½æ¨¡å—', ''),
            version,
            normal_date,
            fields.get('æ›´æ–°ç±»å‹', ''),
            first_level,
            second_level,
            baseline
        ]
        if version:
            filter_items.append(row)
    return filter_items

def get_release_Dict(data):
    if not data or len(data) < 2:
        logging.warning("æ•°æ®ä¸è¶³ï¼Œæ— æ³•ç”Ÿæˆreleaseå­—å…¸")
        return OrderedDict()
    df = pd.DataFrame(data[1:], columns=data[0])
    def version_key(v):
        try:
            parts = v.lstrip('v').split('.')
            return tuple(int(x) for x in parts)
        except Exception:
            return (0,)
    df['ç‰ˆæœ¬æ’åº'] = df['å‘å¸ƒç‰ˆæœ¬'].apply(version_key)
    module_order = {'ç®—åŠ›äº‘': 0, 'å¤§æ¨¡å‹æœåŠ¡å¹³å°': 1, 'è´¹ç”¨ä¸­å¿ƒ': 2, 'ç”¨æˆ·ä¸­å¿ƒ': 3}
    update_type_order = {'æ–°åŠŸèƒ½': 0, 'å¢å¼ºä¼˜åŒ–': 1, 'æ•…éšœä¿®å¤': 2, 'æ— æ›´æ–°ç±»å‹': 3}
    df['æ›´æ–°ç±»å‹'] = df['æ›´æ–°ç±»å‹'].replace({'': 'æ— æ›´æ–°ç±»å‹'})
    df['åŠŸèƒ½æ¨¡å—æ’åº'] = df['åŠŸèƒ½æ¨¡å—'].map(module_order).fillna(99)
    df['æ›´æ–°ç±»å‹æ’åº'] = df['æ›´æ–°ç±»å‹'].map(update_type_order).fillna(99)
    df_sorted = df.sort_values(by=['å‘ç‰ˆæ—¶é—´', 'åŠŸèƒ½æ¨¡å—æ’åº', 'ç‰ˆæœ¬æ’åº', 'æ›´æ–°ç±»å‹æ’åº', 'ä¸€çº§åŠŸèƒ½'],
                               ascending=[False, True, True, True, True])
    result = OrderedDict()
    for _, row in df_sorted.iterrows():
        pub_date = row['å‘ç‰ˆæ—¶é—´']
        module = row['åŠŸèƒ½æ¨¡å—']
        version = row['å‘å¸ƒç‰ˆæœ¬']
        update_type = row['æ›´æ–°ç±»å‹']
        primary_func = row['ä¸€çº§åŠŸèƒ½']
        entry = {
            'äºŒçº§åŠŸèƒ½': row['äºŒçº§åŠŸèƒ½'],
            'åŸºçº¿å‚æ•°': row['åŸºçº¿å‚æ•°']
        }
        result.setdefault(pub_date, OrderedDict()) \
              .setdefault(module, OrderedDict()) \
              .setdefault(version, OrderedDict()) \
              .setdefault(update_type, []).append((primary_func, entry))
    # ç»„å†…ä¸€çº§åŠŸèƒ½æ’åº
    for pub_date in result:
        for module in result[pub_date]:
            for version in result[pub_date][module]:
                for update_type in result[pub_date][module][version]:
                    result[pub_date][module][version][update_type].sort(key=lambda x: x[0])
    return result

def get_release_info(data, filename='rel-notes.md'):
    items = get_items(data)
    result = get_release_Dict(items)
    emoji_map = {
        'æ–°åŠŸèƒ½': 'ğŸš€ æ–°åŠŸèƒ½',
        'å¢å¼ºä¼˜åŒ–': 'âš¡ å¢å¼ºä¼˜åŒ–',
        'æ•…éšœä¿®å¤': 'ğŸ› æ•…éšœä¿®å¤',
        'æ— æ›´æ–°ç±»å‹': ''
    }
    lines = []
    lines.append('---')
    lines.append('hide:')
    lines.append(' - toc')
    lines.append('---\n')
    lines.append('# Release Notes\n')
    lines.append('æœ¬é¡µåˆ—å‡º d.run å„é¡¹åŠŸèƒ½çš„ä¸€äº›é‡è¦å˜æ›´ã€‚\n')
    for pub_date, modules in result.items():
        lines.append(f'## {pub_date}\n')
        for module, versions in modules.items():
            for version, update_types in versions.items():
                lines.append(f'### {module} {version}\n')
                for update_type in ['æ–°åŠŸèƒ½', 'å¢å¼ºä¼˜åŒ–', 'æ•…éšœä¿®å¤', 'æ— æ›´æ–°ç±»å‹']:
                    if update_type in update_types:
                        entries = update_types[update_type]
                        if update_type != 'æ— æ›´æ–°ç±»å‹':
                            lines.append(f'#### {emoji_map[update_type]}\n')
                        for primary_func, entry in entries:
                            baseline = entry['åŸºçº¿å‚æ•°']
                            if baseline:
                                lines.append(f'- [{primary_func}] {baseline}')
                            else:
                                lines.append(f'- [{primary_func}]')
                        lines.append('')
    md_content = '\n'.join(lines)
    try:
        with open(filename, 'w', encoding='utf-8') as f:
            f.write(md_content)
        logging.info("Release noteså·²ä¿å­˜åˆ° %s", filename)
    except Exception as e:
        logging.error("å†™å…¥release notesæ–‡ä»¶å¤±è´¥: %s", e)

def parse_feishu_url(url):
    node_token = None
    table_id = None
    view_id = None
    if not url:
        return node_token, table_id, view_id
    match_node = re.search(r'/wiki/([^/?]+)', url)
    if match_node:
        node_token = match_node.group(1)
    match_table = re.search(r'table=([^&]+)', url)
    if match_table:
        table_id = match_table.group(1)
    match_view = re.search(r'view=([^&]+)', url)
    if match_view:
        view_id = match_view.group(1)
    return node_token, table_id, view_id

if __name__ == "__main__":
    file_out_name = 'docs/zh/docs/rel-notes.md'

    app_id = os.environ.get("APP_ID")
    app_secret = os.environ.get("APP_SECRET")
    url = os.environ.get("URL")
    node_token, table_id, view_id = parse_feishu_url(url) if url else (None, None, None)
    if not all([app_id, app_secret, node_token, table_id, view_id]):
        logging.error("é…ç½®ä¿¡æ¯ä¸å®Œæ•´ï¼Œç¨‹åºé€€å‡º")
        exit(1)

    tenant_access_token = get_tenant_access_token(app_id, app_secret)
    if not tenant_access_token:
        logging.error("è·å–tenant_access_tokenå¤±è´¥ï¼Œç¨‹åºé€€å‡º")
        exit(1)

    node_info = get_node_info(node_token, tenant_access_token)
    if not node_info or 'data' not in node_info or 'node' not in node_info['data']:
        logging.error("è·å–node_infoå¤±è´¥ï¼Œç¨‹åºé€€å‡º")
        exit(1)
    app_token = node_info["data"]["node"].get("obj_token")
    if not app_token:
        logging.error("obj_tokenç¼ºå¤±ï¼Œç¨‹åºé€€å‡º")
        exit(1)

    ori_data = get_sheet_content(app_token, table_id, view_id, tenant_access_token)
    if not ori_data:
        logging.error("è·å–è¡¨æ ¼å†…å®¹å¤±è´¥ï¼Œç¨‹åºé€€å‡º")
        exit(1)
    get_release_info(ori_data, filename=file_out_name)